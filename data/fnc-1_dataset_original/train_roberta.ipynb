{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b38e14a2-7f58-40c4-8142-6a8b13e08a77",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_bodies.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d8dcea76659c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m headlines, bodies, labels = fnc(\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train_stances.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train_bodies.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-d8dcea76659c>\u001b[0m in \u001b[0;36mfnc\u001b[1;34m(path_headlines, path_bodies)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mmap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'agree'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'disagree'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'discuss'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'unrelated'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_bodies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf_8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfb\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Body ID,articleBody\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mbody_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mlines_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_bodies.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from simpletransformers.model import TransformerModel\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "\n",
    "def fnc(path_headlines, path_bodies):\n",
    "\n",
    "    map = {'agree': 0, 'disagree':1, 'discuss':2, 'unrelated':3}\n",
    "\n",
    "    with open(path_bodies, encoding='utf_8') as fb:  # Body ID,articleBody\n",
    "        body_dict = {}\n",
    "        lines_b = csv.reader(fb)\n",
    "        for i, line in enumerate(tqdm(list(lines_b), ncols=80, leave=False)):\n",
    "            if i > 0:\n",
    "                body_id = int(line[0].strip())\n",
    "                body_dict[body_id] = line[1]\n",
    "\n",
    "    with open(path_headlines, encoding='utf_8') as fh: # Headline,Body ID,Stance\n",
    "        lines_h = csv.reader(fh)\n",
    "        h = []\n",
    "        b = []\n",
    "        l = []\n",
    "        for i, line in enumerate(tqdm(list(lines_h), ncols=80, leave=False)):\n",
    "            if i > 0:\n",
    "                body_id = int(line[1].strip())\n",
    "                label = line[2].strip()\n",
    "                if label in map and body_id in body_dict:\n",
    "                    h.append(line[0])\n",
    "                    l.append(map[line[2]])\n",
    "                    b.append(body_dict[body_id])\n",
    "    return h, b, l\n",
    "\n",
    "data_dir = ''\n",
    "headlines, bodies, labels = fnc(\n",
    "    os.path.join(data_dir, 'train_stances.csv'),\n",
    "    os.path.join(data_dir, 'train_bodies.csv')\n",
    ")\n",
    "\n",
    "list_of_tuples = list(zip(headlines, bodies, labels))\n",
    "df = pd.DataFrame(list_of_tuples, columns=['text_a', 'text_b', 'labels'])\n",
    "train_df, val_df = train_test_split(df)\n",
    "labels_val = pd.Series(val_df['labels']).to_numpy()\n",
    "\n",
    "headlines, bodies, labels = fnc(\n",
    "    os.path.join(data_dir, 'competition_test_stances.csv'),\n",
    "    os.path.join(data_dir, 'competition_test_bodies.csv')\n",
    ")\n",
    "\n",
    "list_of_tuples = list(zip(headlines, bodies, labels))\n",
    "test_df = pd.DataFrame(list_of_tuples, columns=['text_a', 'text_b', 'labels'])\n",
    "labels_test = pd.Series(test_df['labels']).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10babe00-3a0f-4e44-bc2c-60d165a2f9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.model import TransformerModel\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "import wandb\n",
    "\n",
    "wandb.init()\n",
    "model = TransformerModel('roberta', 'roberta-base', num_labels=4, args={\n",
    "    'learning_rate':1e-5,\n",
    "    'num_train_epochs': 1,\n",
    "    'reprocess_input_data': True,\n",
    "    'overwrite_output_dir': True,\n",
    "    'process_count': 10,\n",
    "    'train_batch_size': 1,\n",
    "    'eval_batch_size': 1,\n",
    "    'max_seq_length': 64,\n",
    "    'fp16': True,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'tensorboard_dir': '',\n",
    "    'wandb_project': 'fnc_roberta'\n",
    "})\n",
    "\n",
    "model.train_model(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae0664b-d10d-45da-a098-5be815405078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "_, model_outputs_test, _ = model.eval_model(test_df)\n",
    "\n",
    "preds_test = np.argmax(model_outputs_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02101a13-436d-4b1b-bedf-085c7ef93f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def calculate_f1_scores(y_true, y_predicted):\n",
    "    f1_macro = f1_score(y_true, y_predicted, average='macro')\n",
    "    f1_classwise = f1_score(y_true, y_predicted, average=None, labels=[0, 1, 2, 3])\n",
    "\n",
    "    resultstring = \"F1 macro: {:.3f}\".format(f1_macro * 100) + \"% \\n\"\n",
    "    resultstring += \"F1 agree: {:.3f}\".format(f1_classwise[0] * 100) + \"% \\n\"\n",
    "    resultstring += \"F1 disagree: {:.3f}\".format(f1_classwise[1] * 100) + \"% \\n\"\n",
    "    resultstring += \"F1 discuss: {:.3f}\".format(f1_classwise[2] * 100) + \"% \\n\"\n",
    "    resultstring += \"F1 unrelated: {:.3f}\".format(f1_classwise[3] * 100) + \"% \\n\"\n",
    "\n",
    "    return resultstring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47a4382-a9f8-4640-a3ad-23d0ff32892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_f1_scores(preds_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfbb16b-915a-4a30-b411-d6d45b8abcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "LABELS = [0, 1, 2, 3]\n",
    "RELATED = [0, 1, 2]\n",
    "\n",
    "def print_confusion_matrix(cm):\n",
    "    lines = ['CONFUSION MATRIX:']\n",
    "    header = \"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format('', *LABELS)\n",
    "    line_len = len(header)\n",
    "    lines.append(\"-\"*line_len)\n",
    "    lines.append(header)\n",
    "    lines.append(\"-\"*line_len)\n",
    "    hit = 0\n",
    "    total = 0\n",
    "    for i, row in enumerate(cm):\n",
    "        hit += row[i]\n",
    "        total += sum(row)\n",
    "        lines.append(\"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format(LABELS[i], *row))\n",
    "        lines.append(\"-\"*line_len)\n",
    "    lines.append(\"ACCURACY: {:.3f}\".format((hit / total)*100) + \"%\")\n",
    "    print('\\n'.join(lines))\n",
    "\n",
    "def fnc_score_cm(predicted_labels, target):\n",
    "    score = 0.0\n",
    "    cm = [[0, 0, 0, 0],\n",
    "          [0, 0, 0, 0],\n",
    "          [0, 0, 0, 0],\n",
    "          [0, 0, 0, 0]]\n",
    "    for i, (g, t) in enumerate(zip(predicted_labels, target)):\n",
    "            if g == t:\n",
    "                score += 0.25\n",
    "                if g != 3:\n",
    "                    score += 0.50\n",
    "            if g in RELATED and t in RELATED:\n",
    "                score += 0.25\n",
    "\n",
    "            cm[g][t] += 1\n",
    "    return score,  cm\n",
    "\n",
    "fnc_score, cm_test = fnc_score_cm(preds_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633fe3cb-a42e-4cb8-9de5-9936f566d54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRelative FNC Score: {:.3f}\".format(100/13204.75*fnc_score) + \"% \\n\")\n",
    "print_confusion_matrix(cm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7774970a-44af-4ebe-bae3-69eaaf721dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7f6452-26ee-4eaa-8566-5a836095c6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(disp.plot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111de6ce-82f9-46a0-9e0f-31188a888ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "eval_report = classification_report(labels_test, preds_test, target_names=LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07b59a5-20c8-4c4d-9d30-44ea2c5b414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test report', eval_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed2e9a6-4a73-4aaa-9c95-fd62abaf5c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(preds_test, labels_test)\n",
    "print(fpr, tpr, threshold)\n",
    "\n",
    "auc1 = auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, color = 'orange', label = 'AUC = %0.2f' % auc1)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693fa094-ddf4-49a8-b354-e39d05def7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plt.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e8215d-ec11-4ceb-b2f4-6feb2c989313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('train_roberta_0110.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
