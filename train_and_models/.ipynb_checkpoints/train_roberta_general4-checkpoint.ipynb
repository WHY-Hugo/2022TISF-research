{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38e14a2-7f58-40c4-8142-6a8b13e08a77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from simpletransformers.model import TransformerModel\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "\n",
    "def fnc(path_headlines, path_bodies):\n",
    "\n",
    "    map = {'agree': 0, 'disagree':1, 'discuss':2, 'unrelated':3}\n",
    "\n",
    "    with open(path_bodies, encoding='utf_8') as fb:  # Body ID,articleBody\n",
    "        body_dict = {}\n",
    "        lines_b = csv.reader(fb)\n",
    "        for i, line in enumerate(tqdm(list(lines_b), ncols=80, leave=False)):\n",
    "            if i > 0:\n",
    "                body_id = int(line[0].strip())\n",
    "                body_dict[body_id] = line[1]\n",
    "\n",
    "    with open(path_headlines, encoding='utf_8') as fh: # Headline,Body ID,Stance\n",
    "        lines_h = csv.reader(fh)\n",
    "        h = []\n",
    "        b = []\n",
    "        l = []\n",
    "        for i, line in enumerate(tqdm(list(lines_h), ncols=80, leave=False)):\n",
    "            if i > 0:\n",
    "                body_id = int(line[1].strip())\n",
    "                label = line[2].strip()\n",
    "                if label in map and body_id in body_dict:\n",
    "                    h.append(line[0])\n",
    "                    l.append(map[line[2]])\n",
    "                    b.append(body_dict[body_id])\n",
    "    return h, b, l\n",
    "\n",
    "data_dir = ''\n",
    "headlines, bodies, labels = fnc(\n",
    "    os.path.join(data_dir, 'train_stances.csv'),\n",
    "    os.path.join(data_dir, 'train_bodies.csv')\n",
    ")\n",
    "\n",
    "list_of_tuples = list(zip(headlines, bodies, labels))\n",
    "df = pd.DataFrame(list_of_tuples, columns=['text_a', 'text_b', 'labels'])\n",
    "train_df, val_df = train_test_split(df)\n",
    "labels_val = pd.Series(val_df['labels']).to_numpy()\n",
    "\n",
    "headlines, bodies, labels = fnc(\n",
    "    os.path.join(data_dir, 'competition_test_stances.csv'),\n",
    "    os.path.join(data_dir, 'competition_test_bodies.csv')\n",
    ")\n",
    "\n",
    "list_of_tuples = list(zip(headlines, bodies, labels))\n",
    "test_df = pd.DataFrame(list_of_tuples, columns=['text_a', 'text_b', 'labels'])\n",
    "labels_test = pd.Series(test_df['labels']).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc43ccb5-35e7-4c02-8e56-4e383fca1cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login(key='8bc03e3a805535fd278efa47fca6bf2ca5793823')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10babe00-3a0f-4e44-bc2c-60d165a2f9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.model import TransformerModel\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "\n",
    "\n",
    "wandb.init()\n",
    "model = TransformerModel('roberta', 'roberta-base', num_labels=4, args={\n",
    "    'learning_rate':1e-5,\n",
    "    'num_train_epochs': 5,\n",
    "    'reprocess_input_data': True,\n",
    "    'overwrite_output_dir': True,\n",
    "    'process_count': 10,\n",
    "    'train_batch_size': 4,\n",
    "    'eval_batch_size': 4,\n",
    "    'max_seq_length': 512,\n",
    "    'fp16': True,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'tensorboard_dir': '',\n",
    "    'wandb_project': 'fnc_roberta_0116_general4'\n",
    "})\n",
    "\n",
    "model.train_model(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae0664b-d10d-45da-a098-5be815405078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "_, model_outputs_test, _ = model.eval_model(test_df)\n",
    "\n",
    "preds_test = np.argmax(model_outputs_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02101a13-436d-4b1b-bedf-085c7ef93f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def calculate_f1_scores(y_true, y_predicted):\n",
    "    f1_macro = f1_score(y_true, y_predicted, average='macro')\n",
    "    f1_classwise = f1_score(y_true, y_predicted, average=None, labels=[0, 1, 2, 3])\n",
    "\n",
    "    resultstring = \"F1 macro: {:.3f}\".format(f1_macro * 100) + \"% \\n\"\n",
    "    resultstring += \"F1 agree: {:.3f}\".format(f1_classwise[0] * 100) + \"% \\n\"\n",
    "    resultstring += \"F1 disagree: {:.3f}\".format(f1_classwise[1] * 100) + \"% \\n\"\n",
    "    resultstring += \"F1 discuss: {:.3f}\".format(f1_classwise[2] * 100) + \"% \\n\"\n",
    "    resultstring += \"F1 unrelated: {:.3f}\".format(f1_classwise[3] * 100) + \"% \\n\"\n",
    "\n",
    "    return resultstring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47a4382-a9f8-4640-a3ad-23d0ff32892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calculate_f1_scores(preds_test, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfbb16b-915a-4a30-b411-d6d45b8abcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "LABELS = [0, 1, 2, 3]\n",
    "RELATED = [0, 1, 2]\n",
    "\n",
    "def print_confusion_matrix(cm):\n",
    "    lines = ['CONFUSION MATRIX:']\n",
    "    header = \"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format('', *LABELS)\n",
    "    line_len = len(header)\n",
    "    lines.append(\"-\"*line_len)\n",
    "    lines.append(header)\n",
    "    lines.append(\"-\"*line_len)\n",
    "    hit = 0\n",
    "    total = 0\n",
    "    for i, row in enumerate(cm):\n",
    "        hit += row[i]\n",
    "        total += sum(row)\n",
    "        lines.append(\"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format(LABELS[i], *row))\n",
    "        lines.append(\"-\"*line_len)\n",
    "    lines.append(\"ACCURACY: {:.3f}\".format((hit / total)*100) + \"%\")\n",
    "    print('\\n'.join(lines))\n",
    "\n",
    "def fnc_score_cm(predicted_labels, target):\n",
    "    score = 0.0\n",
    "    cm = [[0, 0, 0, 0],\n",
    "          [0, 0, 0, 0],\n",
    "          [0, 0, 0, 0],\n",
    "          [0, 0, 0, 0]]\n",
    "    for i, (g, t) in enumerate(zip(predicted_labels, target)):\n",
    "            if g == t:\n",
    "                score += 0.25\n",
    "                if g != 3:\n",
    "                    score += 0.50\n",
    "            if g in RELATED and t in RELATED:\n",
    "                score += 0.25\n",
    "\n",
    "            cm[g][t] += 1\n",
    "    return score,  cm\n",
    "\n",
    "fnc_score, cm_test = fnc_score_cm(preds_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633fe3cb-a42e-4cb8-9de5-9936f566d54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRelative FNC Score: {:.3f}\".format(100/13204.75*fnc_score) + \"% \\n\")\n",
    "print(print_confusion_matrix(cm_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7774970a-44af-4ebe-bae3-69eaaf721dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(preds_test, labels_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7f6452-26ee-4eaa-8566-5a836095c6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(disp.plot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111de6ce-82f9-46a0-9e0f-31188a888ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "eval_report = classification_report(labels_test, preds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07b59a5-20c8-4c4d-9d30-44ea2c5b414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test report\\n', eval_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e8215d-ec11-4ceb-b2f4-6feb2c989313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('fnc_roberta_0116_general4.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
