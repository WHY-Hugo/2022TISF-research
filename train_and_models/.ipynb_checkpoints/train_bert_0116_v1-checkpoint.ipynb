{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b8b0f0-fd31-4cb0-b3ab-b6d93fdff734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import logging\n",
    "\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "LABELS = [0, 1, 2, 3]\n",
    "RELATED = [0, 1, 2]\n",
    "\n",
    "def fnc(path_headlines, path_bodies):\n",
    "\n",
    "    map = {'agree': 0, 'disagree':1, 'discuss':2, 'unrelated':3}\n",
    "\n",
    "    with open(path_bodies, encoding='utf_8') as fb:  # Body ID,articleBody\n",
    "        body_dict = {}\n",
    "        lines_b = csv.reader(fb)\n",
    "        for i, line in enumerate(tqdm(list(lines_b), ncols=80, leave=False)):\n",
    "            if i > 0:\n",
    "                body_id = int(line[0].strip())\n",
    "                body_dict[body_id] = line[1]\n",
    "\n",
    "    with open(path_headlines, encoding='utf_8') as fh: # Headline,Body ID,Stance\n",
    "        lines_h = csv.reader(fh)\n",
    "        h = []\n",
    "        b = []\n",
    "        l = []\n",
    "        for i, line in enumerate(tqdm(list(lines_h), ncols=80, leave=False)):\n",
    "            if i > 0:\n",
    "                body_id = int(line[1].strip())\n",
    "                label = line[2].strip()\n",
    "                if label in map and body_id in body_dict:\n",
    "                    h.append(line[0])\n",
    "                    l.append(map[line[2]])\n",
    "                    b.append(body_dict[body_id])\n",
    "    return h, b, l\n",
    "\n",
    "data_dir = ''\n",
    "headlines, bodies, labels = fnc(\n",
    "    os.path.join(data_dir, 'train_stances.csv'),\n",
    "    os.path.join(data_dir, 'train_bodies.csv')\n",
    ")\n",
    "\n",
    "list_of_tuples = list(zip(headlines, bodies, labels))\n",
    "df = pd.DataFrame(list_of_tuples, columns=['text_a', 'text_b', 'labels'])\n",
    "train_df, val_df = train_test_split(df)\n",
    "labels_val = pd.Series(val_df['labels']).to_numpy()\n",
    "\n",
    "headlines, bodies, labels = fnc(\n",
    "    os.path.join(data_dir, 'competition_test_stances.csv'),\n",
    "    os.path.join(data_dir, 'competition_test_bodies.csv')\n",
    ")\n",
    "\n",
    "list_of_tuples = list(zip(headlines, bodies, labels))\n",
    "test_df = pd.DataFrame(list_of_tuples, columns=['text_a', 'text_b', 'labels'])\n",
    "labels_test = pd.Series(test_df['labels']).to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "def calculate_f1_scores(y_true, y_predicted):\n",
    "    f1_macro = f1_score(y_true, y_predicted, average='macro')\n",
    "    f1_classwise = f1_score(y_true, y_predicted, average=None, labels=[0, 1, 2, 3])\n",
    "\n",
    "    resultstring = \"F1 macro: {:.3f}\".format(f1_macro * 100) + \"% \\n\"\n",
    "    resultstring += \"F1 agree: {:.3f}\".format(f1_classwise[0] * 100) + \"% \\n\"\n",
    "    resultstring += \"F1 disagree: {:.3f}\".format(f1_classwise[1] * 100) + \"% \\n\"\n",
    "    resultstring += \"F1 discuss: {:.3f}\".format(f1_classwise[2] * 100) + \"% \\n\"\n",
    "    resultstring += \"F1 unrelated: {:.3f}\".format(f1_classwise[3] * 100) + \"% \\n\"\n",
    "\n",
    "    return resultstring\n",
    "\n",
    "\n",
    "def print_confusion_matrix(cm):\n",
    "    lines = ['CONFUSION MATRIX:']\n",
    "    header = \"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format('', *LABELS)\n",
    "    line_len = len(header)\n",
    "    lines.append(\"-\"*line_len)\n",
    "    lines.append(header)\n",
    "    lines.append(\"-\"*line_len)\n",
    "    hit = 0\n",
    "    total = 0\n",
    "    for i, row in enumerate(cm):\n",
    "        hit += row[i]\n",
    "        total += sum(row)\n",
    "        lines.append(\"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format(LABELS[i], *row))\n",
    "        lines.append(\"-\"*line_len)\n",
    "    lines.append(\"ACCURACY: {:.3f}\".format((hit / total)*100) + \"%\")\n",
    "    print('\\n'.join(lines))\n",
    "\n",
    "def fnc_score_cm(predicted_labels, target):\n",
    "    score = 0.0\n",
    "    cm = [[0, 0, 0, 0],\n",
    "          [0, 0, 0, 0],\n",
    "          [0, 0, 0, 0],\n",
    "          [0, 0, 0, 0]]\n",
    "    for i, (g, t) in enumerate(zip(predicted_labels, target)):\n",
    "            if g == t:\n",
    "                score += 0.25\n",
    "                if g != 3:\n",
    "                    score += 0.50\n",
    "            if g in RELATED and t in RELATED:\n",
    "                score += 0.25\n",
    "\n",
    "            cm[g][t] += 1\n",
    "    return score,  cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b614cc-4d3c-481c-8f96-14d2f01ffbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key='8bc03e3a805535fd278efa47fca6bf2ca5793823')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a643efd-32af-47a5-bff6-c5f70beead15",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"bayes\",  # grid, random\n",
    "    \"metric\": {\"name\": \"train_loss\", \"goal\": \"minimize\"},\n",
    "    \"parameters\": {\n",
    "        \"num_train_epochs\": {\"values\": [3, 5, 7]},\n",
    "        \"learning_rate\": {\"min\": 1e-5, \n",
    "                          \"max\": 4e-4},\n",
    "        'dropout': {'values': [0.3, 0.4, 0.5, 0.6, 0.7]},\n",
    "        \"optimizer\": {\"values\": ['AdamW']},\n",
    "        \"train_batch_size\": {\"min\": 4, \n",
    "                             \"max\":12, \n",
    "                             \"distribution\": \"int_uniform\"},\n",
    "        \"fc_layer_size\": {'values': [128, 256, 512]}\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"fnc_bert_0116_v1\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b94773-f751-418f-9345-314bc23f89e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.model import TransformerModel\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "\n",
    "def train():\n",
    "    # Initialize a new wandb run\n",
    "    wandb.init()\n",
    "\n",
    "    # Create a TransformerModel\n",
    "    model = TransformerModel('bert', 'bert-base-uncased', num_labels=4, use_cuda=True, sweep_config=wandb.config, args={\n",
    "        'reprocess_input_data': True,\n",
    "        'overwrite_output_dir': True,\n",
    "        'process_count': 10,\n",
    "        'eval_batch_size': 4,\n",
    "        'max_seq_length': 512,\n",
    "        'fp16': True,\n",
    "        'gradient_accumulation_steps': 1,\n",
    "        'tensorboard_dir': '',\n",
    "        'evaluate_during_training': True,\n",
    "        'manual_seed': 4,\n",
    "        'use_multiprocessing': True\n",
    "    })\n",
    "\n",
    "    # Train the model\n",
    "    model.train_model(train_df, eval_df=test_df)\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval_model(test_df)\n",
    "\n",
    "    # Sync wandb\n",
    "    wandb.join()\n",
    "\n",
    "    _, model_outputs_test, _ = model.eval_model(test_df)\n",
    "    preds_test = np.argmax(model_outputs_test, axis=1)\n",
    "    print(calculate_f1_scores(preds_test, labels_test))\n",
    "    fnc_score, cm_test = fnc_score_cm(preds_test, labels_test)\n",
    "    print(\"\\nRelative FNC Score: {:.3f}\".format(100/13204.75*fnc_score) + \"% \\n\")\n",
    "    eval_report = classification_report(labels_test, preds_test)\n",
    "    print('Test report\\n', eval_report)\n",
    "    cm = confusion_matrix(preds_test, labels_test)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=LABELS)\n",
    "    print(disp.plot())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ec88d5-884c-41a0-a2eb-831c95b056fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, train, count=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
